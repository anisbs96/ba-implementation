{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Log Generalization and Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Log Configuration --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logs Path\n",
    "path_to_logs = '../Collected_Logs/'\n",
    "\n",
    "# Number of faults\n",
    "num_faults = 4\n",
    "\n",
    "# Mapping faults and components\n",
    "fault_to_component_mapping = {\n",
    "    'fault_1': 'flink-master',\n",
    "    'fault_2': 'flink-worker',\n",
    "    'fault_3': 'kafka-broker',\n",
    "    'fault_4': 'functions'\n",
    "}\n",
    "component_mapping = {\n",
    "    '/components-master-1': 'flink-master',\n",
    "    '/components-worker-1': 'flink-worker',\n",
    "    '/components-kafka-1': 'kafka-broker',\n",
    "    '/components-functions-1': 'functions',\n",
    "    '/components-console-1': 'kafka-console'\n",
    "}\n",
    "\n",
    "# Phase time ranges\n",
    "phase_ranges = {\n",
    "    'flink-master': [('2024-11-08 22:50:04','2024-11-08 22:56:05'), ('2024-11-08 22:58:06','2024-11-08 23:04:07'), ('2024-11-08 23:06:08','2024-11-08 23:12:08'), ('2024-11-08 23:14:10','2024-11-08 23:20:10')],\n",
    "    'flink-worker': [('2024-11-08 22:50:05', '2024-11-08 22:52:06'), ('2024-11-08 22:55:06', '2024-11-08 22:57:07'), ('2024-11-08 23:00:08', '2024-11-08 23:02:08'), ('2024-11-08 23:05:09', '2024-11-08 23:07:09'), ('2024-11-08 23:10:10', '2024-11-08 23:12:10'), ('2024-11-08 23:15:11', '2024-11-08 23:17:12')],\n",
    "    'kafka-broker': [('2024-11-08 22:50:06', '2024-11-08 22:52:07'), ('2024-11-08 22:55:07', '2024-11-08 22:57:08'), ('2024-11-08 23:00:08', '2024-11-08 23:02:09'), ('2024-11-08 23:05:09', '2024-11-08 23:07:10'), ('2024-11-08 23:10:11', '2024-11-08 23:12:12'), ('2024-11-08 23:15:12', '2024-11-08 23:17:13')],\n",
    "    'functions': [('2024-11-08 22:50:07', '2024-11-08 22:52:07'), ('2024-11-08 22:55:07', '2024-11-08 22:57:08'), ('2024-11-08 23:00:08', '2024-11-08 23:02:09'), ('2024-11-08 23:05:09', '2024-11-08 23:07:10'), ('2024-11-08 23:10:10', '2024-11-08 23:12:10'), ('2024-11-08 23:15:11', '2024-11-08 23:17:11')],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Log Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract All Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'fault-target' attribute on all logs files\n",
    "def load_logs(normal_file_path, fault_file_pattern, num_faults):\n",
    "    normal_logs = pd.read_csv(normal_file_path)\n",
    "    normal_logs['fault-target'] = 'normal'\n",
    "\n",
    "    all_logs = [normal_logs]\n",
    "    \n",
    "    for i in range(1, num_faults + 1):\n",
    "        fault_file = fault_file_pattern.format(i)\n",
    "        if os.path.exists(fault_file):\n",
    "            fault_logs = pd.read_csv(fault_file)\n",
    "            fault_logs['fault-target'] = f'fault_{i}' \n",
    "            all_logs.append(fault_logs)\n",
    "        else:\n",
    "            print(f\"File {fault_file} not found.\")\n",
    "    \n",
    "    combined_logs = pd.concat(all_logs, ignore_index=True)\n",
    "    return combined_logs\n",
    "\n",
    "# Usage\n",
    "normal_file_path = path_to_logs + 'normal.csv'\n",
    "fault_file_pattern = path_to_logs + 'fault_{}.csv'\n",
    "combined_logs_df = load_logs(normal_file_path, fault_file_pattern, num_faults)\n",
    "\n",
    "combined_logs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Relevant Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the logs and only keep the needed columns\n",
    "def filter_and_reorder_columns(logs_df):\n",
    "    needed_columns = ['timestamp', 'fault-target', 'jsonPayload.container.name', 'jsonPayload.message']\n",
    "    \n",
    "    reordered_logs_df = logs_df[needed_columns]\n",
    "    \n",
    "    return reordered_logs_df\n",
    "\n",
    "# Usage\n",
    "attributes_logs_df = filter_and_reorder_columns(combined_logs_df).rename(columns={'jsonPayload.container.name': 'component'})\n",
    "\n",
    "attributes_logs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Logs with missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove incomplete logs \n",
    "def drop_rows_with_missing_values(logs_df):\n",
    "    logs_df_cleaned = logs_df.dropna(how='any')\n",
    "\n",
    "    return logs_df_cleaned\n",
    "\n",
    "# Usage\n",
    "processed_logs_df = drop_rows_with_missing_values(attributes_logs_df)\n",
    "processed_logs_df['component'] = processed_logs_df['component'].replace(component_mapping)\n",
    "processed_logs_df['fault-target'] = processed_logs_df['fault-target'].replace(fault_to_component_mapping)\n",
    "\n",
    "processed_logs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export result to CSV file\n",
    "processed_logs_df.to_csv('../Results/Log_Generalization/4.4.1-collected_logs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Log Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Severity Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'severity_level' attribute\n",
    "# Extracted and removed from 'jsonPayload.message'\n",
    "def extract_and_remove_severity_level(logs_df):\n",
    "    severity_pattern = r'\\b(FATAL|ERROR|WARN|WARNING|INFO)\\b'\n",
    "    \n",
    "    def clean_message_and_extract_severity(message):\n",
    "        if isinstance(message, str):\n",
    "            match = re.search(severity_pattern, message, re.IGNORECASE)\n",
    "            if match:\n",
    "                severity = match.group(0).upper()\n",
    "                if severity == 'WARNING':\n",
    "                    return 'WARN', message\n",
    "                else:\n",
    "                    return severity, message\n",
    "            else:\n",
    "                return None, None\n",
    "        else:\n",
    "            return None, None\n",
    "    \n",
    "    logs_df['severity_level'], logs_df['jsonPayload.message'] = zip(*logs_df['jsonPayload.message'].apply(clean_message_and_extract_severity))\n",
    "    \n",
    "    # Remove incomplete logs\n",
    "    logs_df = logs_df.dropna(subset=['severity_level', 'jsonPayload.message'])\n",
    "\n",
    "    return logs_df\n",
    "\n",
    "# Usage:\n",
    "severity_logs_df = extract_and_remove_severity_level(processed_logs_df)\n",
    "\n",
    "severity_logs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove variable Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 'jsonPayload.message' from dynamic content\n",
    "def template_all_logs(logs_df):\n",
    "    patterns = {\n",
    "        'severity' : r'\\b(FATAL|ERROR|WARN|WARNING|INFO)\\b',\n",
    "        'timestamp': r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}(,\\d{3})?',\n",
    "        'special': r'[@#~$]',\n",
    "        'uuid': r'[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}',  \n",
    "        'random_string': r'-[a-zA-Z0-9]{6}',\n",
    "        'hex_id': r'@[a-fA-F0-9]{8}',\n",
    "        'sender_id': r'\\[@akka\\.tcp:@:\\w+\\]',\n",
    "        'recipient_id': r'\\[@akka:#\\]',\n",
    "        'path': r'/[\\w\\-/.]+',\n",
    "        'ip_address': r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b',\n",
    "        'port': r'\\b\\d{2,5}\\b',\n",
    "        'number': r'\\b\\d+(\\.\\d+)?\\b',\n",
    "        'error_code': r'\\b[A-Z0-9]+\\b',\n",
    "        'mac_address': r'(?:[0-9A-Fa-f]{2}[:-]){5}(?:[0-9A-Fa-f]{2})',\n",
    "        'jar_details': r'\\[.*?\\.jar:.*?\\]',\n",
    "        'method_call': r'\\$\\w+\\.\\w+\\(.java:\\)',\n",
    "        'special_in_address': r'\\(registry, proxy, [A-Za-z0-9]+\\)',\n",
    "        'job_id': r'\\b(?=[A-Za-z]*\\d)(?=\\d*[A-Za-z])[A-Za-z0-9]+\\b',\n",
    "        'job_id_2': r'\\b(?=[A-Za-z]*\\d)(?=\\d*[A-Za-z])[A-Za-z0-9]{8,}\\b',\n",
    "        'topic_id_': r'_[A-Za-z0-9]+(?=,|\\.|$)',\n",
    "        'task_id_': r'\\b[A-Za-z0-9]{32}\\b'\n",
    "    }\n",
    "\n",
    "    def automatic_log_templating(log_message):\n",
    "        for placeholder, pattern in patterns.items():\n",
    "            log_message = re.sub(pattern, '', log_message)\n",
    "        return log_message\n",
    "\n",
    "    logs_df['templated_message'] = logs_df['jsonPayload.message'].apply(automatic_log_templating)\n",
    "\n",
    "    return logs_df\n",
    "\n",
    "# Usage:\n",
    "templated_logs_df = template_all_logs(severity_logs_df)\n",
    "if 'jsonPayload.message' in templated_logs_df.columns:\n",
    "    templated_logs_df = templated_logs_df.drop(columns=['jsonPayload.message'])\n",
    "\n",
    "templated_logs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export result to CSV file\n",
    "templated_logs_df.to_csv('../Results/Log_Generalization/4.4.2-generalized_logs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Log Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Normal Logs with distinct MessagePayload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract single message occurence in normal logs\n",
    "def extract_distinct_normal_logs(logs_df):\n",
    "    normal_logs_df = logs_df[logs_df['fault-target'] == 'normal']\n",
    "    \n",
    "    distinct_normal_logs = normal_logs_df.drop_duplicates(subset=['templated_message'])\n",
    "    \n",
    "    return distinct_normal_logs\n",
    "\n",
    "# Usage:\n",
    "distinct_normal_logs_df = extract_distinct_normal_logs(templated_logs_df)\n",
    "\n",
    "distinct_normal_logs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract fault Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fault logs\n",
    "def remove_normal_logs(logs_df):\n",
    "    fault_only_logs_df = logs_df[logs_df['fault-target'] != 'normal']\n",
    "    \n",
    "    return fault_only_logs_df\n",
    "\n",
    "# Usage\n",
    "fault_logs_df = remove_normal_logs(templated_logs_df)\n",
    "\n",
    "fault_logs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove fault Logs that appear in Normal Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract faut logs that do not exist in normal logs\n",
    "def remove_fault_logs_present_in_normal(distinct_normal_logs, fault_logs):\n",
    "    distinct_normal_messages = distinct_normal_logs['templated_message'].unique()\n",
    "    \n",
    "    fault_logs_filtered_df = fault_logs[~fault_logs['templated_message'].isin(distinct_normal_messages)]\n",
    "    \n",
    "    return fault_logs_filtered_df\n",
    "\n",
    "# Usage:\n",
    "diff_logs_df = remove_fault_logs_present_in_normal(distinct_normal_logs_df, fault_logs_df)\n",
    "\n",
    "diff_logs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export result to CSV file\n",
    "diff_logs_df.to_csv('../Results/Log_Generalization/4.4.3-difference_logs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Log Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_phase_flags(logs_df, phase_ranges):\n",
    "# Set 'phase' attribute\n",
    "# crash or recovery\n",
    "    logs_df['timestamp'] = pd.to_datetime(logs_df['timestamp'], format='%Y-%m-%dT%H:%M:%S.%fZ', utc=True)\n",
    "    \n",
    "    def determine_phase(timestamp, fault_type):\n",
    "        ranges = phase_ranges.get(fault_type, [])\n",
    "        for i, (crash_start, crash_end) in enumerate(ranges):\n",
    "            crash_start = pd.Timestamp(crash_start)\n",
    "            crash_end = pd.Timestamp(crash_end)\n",
    "\n",
    "            if crash_start.tzinfo is None:\n",
    "                crash_start = crash_start.tz_localize('UTC')\n",
    "            if crash_end.tzinfo is None:\n",
    "                crash_end = crash_end.tz_localize('UTC')\n",
    "            \n",
    "            if crash_start <= timestamp <= crash_end:\n",
    "                return 'crash'\n",
    "\n",
    "            if i < len(ranges) - 1:\n",
    "                next_crash_start = pd.Timestamp(ranges[i + 1][0])\n",
    "                if next_crash_start.tzinfo is None:\n",
    "                    next_crash_start = next_crash_start.tz_localize('UTC')\n",
    "                    \n",
    "                if crash_end < timestamp < next_crash_start:\n",
    "                    return 'recover'\n",
    "            else:\n",
    "                if crash_end < timestamp:\n",
    "                    return 'recover'\n",
    "        return 'recover'\n",
    "\n",
    "    logs_df['phase'] = logs_df.apply(\n",
    "        lambda row: determine_phase(row['timestamp'], row['fault-target']),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return logs_df\n",
    "\n",
    "# Usage\n",
    "phases_logs_df = add_phase_flags(diff_logs_df, phase_ranges)\n",
    "\n",
    "phases_logs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate log frequency change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set frequency related attributes (frequency, frequency_phase, frequency_change)\n",
    "def calculate_consolidated_frequency_change(logs_df, time_column='timestamp', time_window='H'):\n",
    "    logs_df[time_column] = pd.to_datetime(logs_df[time_column])\n",
    "    \n",
    "    overall_frequency_df = logs_df.groupby(['fault-target', 'component', 'severity_level', 'templated_message']).size().reset_index(name='frequency')\n",
    "    \n",
    "    logs_df.set_index(time_column, inplace=True)\n",
    "    \n",
    "    frequency_by_time = logs_df.groupby([\n",
    "        pd.Grouper(freq=time_window),\n",
    "        'fault-target',\n",
    "        'component',\n",
    "        'severity_level',\n",
    "        'templated_message',\n",
    "        'phase'\n",
    "    ]).size().reset_index(name='frequency_phase')\n",
    "    \n",
    "    frequency_by_time['frequency_change'] = frequency_by_time.groupby([\n",
    "        'fault-target', 'component', 'severity_level', 'templated_message', 'phase'\n",
    "    ])['frequency_phase'].diff().fillna(0)\n",
    "    \n",
    "    consolidated_df = frequency_by_time.drop(columns=[time_column]).groupby(\n",
    "        ['fault-target', 'component', 'severity_level', 'templated_message', 'phase']\n",
    "    ).agg({\n",
    "        'frequency_phase': 'sum',\n",
    "        'frequency_change': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    result_df = pd.merge(consolidated_df, overall_frequency_df, on=['fault-target', 'component', 'severity_level', 'templated_message'], how='left')\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Usage\n",
    "frequency_change_logs_df = calculate_consolidated_frequency_change(phases_logs_df, time_window='10T')\n",
    "\n",
    "frequency_change_logs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'component-specific' attribute\n",
    "# True if log appears in only one component, else False\n",
    "def add_component_specific_column(logs_df):\n",
    "    component_counts = logs_df.groupby('templated_message')['component'].nunique().reset_index(name='component_count')\n",
    "    \n",
    "    logs_df = logs_df.merge(component_counts, on='templated_message', how='left')\n",
    "    \n",
    "    logs_df['component-specific'] = logs_df['component_count'] == 1\n",
    "    \n",
    "    # Remove 'component_count' column\n",
    "    logs_df.drop(columns=['component_count'], inplace=True)\n",
    "    \n",
    "    return logs_df\n",
    "\n",
    "# Usage\n",
    "component_specific_logs_df = add_component_specific_column(frequency_change_logs_df)\n",
    "\n",
    "component_specific_logs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fault-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'specific' attribute \n",
    "# True if the log appears in only one fault scenario, else False\n",
    "def add_fault_specific_column(logs_df):\n",
    "    fault_counts = logs_df.groupby('templated_message')['fault-target'].nunique().reset_index(name='fault_count')\n",
    "    \n",
    "    logs_df = logs_df.merge(fault_counts, on='templated_message', how='left')\n",
    "    \n",
    "    logs_df['fault-specific'] = logs_df['fault_count'] == 1\n",
    "    \n",
    "    # Remove 'fault_count' column\n",
    "    logs_df.drop(columns=['fault_count'], inplace=True)\n",
    "\n",
    "    return logs_df\n",
    "\n",
    "# Usage:\n",
    "fault_specific_logs_df = add_fault_specific_column(component_specific_logs_df)\n",
    "\n",
    "fault_specific_logs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'rare' attribute\n",
    "def add_rare_column(logs_df):\n",
    "    logs_df['rare'] = (\n",
    "        (logs_df['component-specific'] == True) &\n",
    "        (logs_df['fault-specific'] == True) &\n",
    "        (logs_df['frequency'] == 1)\n",
    "    )\n",
    "    \n",
    "    return logs_df\n",
    "\n",
    "# Usage:\n",
    "rare_logs_df = add_rare_column(fault_specific_logs_df)\n",
    "\n",
    "rare_logs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'direct_cause' attribute\n",
    "# True if the 'fault-target' and 'component' attributes match, else False\n",
    "def add_direct_cause_column(logs_df, fault_to_component_mapping):\n",
    "    def determine_direct_cause(row):\n",
    "        fault_type = row['fault-target']\n",
    "        component = row['component']\n",
    "        \n",
    "        return component == fault_type\n",
    "\n",
    "    logs_df['direct_cause'] = logs_df.apply(determine_direct_cause, axis=1)\n",
    "    \n",
    "    return logs_df\n",
    "\n",
    "# Usage\n",
    "cause_logs_df = add_direct_cause_column(rare_logs_df, fault_to_component_mapping)\n",
    "\n",
    "cause_logs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Log Summary --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the logs and only keep the needed attributes\n",
    "def adjust_columns(logs_df):    \n",
    "    desired_columns = ['severity_level','templated_message', 'direct_cause', 'rare' ,'fault-target', 'fault-specific', 'component',  'component-specific',\n",
    "                       'phase', 'frequency_change', 'frequency', 'frequency_phase']\n",
    "    \n",
    "    logs_df = logs_df[desired_columns]\n",
    "    \n",
    "    return logs_df\n",
    "\n",
    "# Usage\n",
    "generalized_logs_df = adjust_columns(cause_logs_df)\n",
    "\n",
    "generalized_logs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export result to CSV file\n",
    "generalized_logs_df.to_csv('../Results/Log_Generalization/4.4.4-enriched_logs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
