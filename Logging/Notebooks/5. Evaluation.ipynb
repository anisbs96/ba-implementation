{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logs Path\n",
    "path_to_logs = '../Results/'\n",
    "\n",
    "# Load the logs from CSV files\n",
    "collected_logs_df = pd.read_csv(path_to_logs + 'Log_Generalization/4.4.1-collected_logs.csv')\n",
    "templated_logs_df = pd.read_csv(path_to_logs + 'Log_Generalization/4.4.2-generalized_logs.csv')\n",
    "difference_logs_df = pd.read_csv(path_to_logs + 'Log_Generalization/4.4.3-difference_logs.csv')\n",
    "generalized_logs_df = pd.read_csv(path_to_logs + 'Log_Generalization/4.4.4-enriched_logs.csv')\n",
    "determination_logs_df = pd.read_csv(path_to_logs + 'Anomaly_Determination/4.5-determination_logs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count logs by attribute\n",
    "def count_logs(logs_df, attribute):\n",
    "    grouped_logs = logs_df.groupby(attribute).size().reset_index(name='count')\n",
    "    return grouped_logs\n",
    "\n",
    "# Group logs by 2 attributes and count\n",
    "def group_by_and_count(logs_df, attribute1,attribute2):\n",
    "    grouped_logs = logs_df.groupby([attribute1, attribute2]).size().reset_index(name='count')\n",
    "    return grouped_logs\n",
    "\n",
    "# Group logs by 2 attributes and sum\n",
    "def group_by_and_sum(logs_df, attribute1,attribute2):\n",
    "    grouped_logs = logs_df.groupby([attribute1, attribute2]).sum().reset_index(name='sum')\n",
    "    return grouped_logs\n",
    "\n",
    "# Group logs by 3 attributes and count\n",
    "def group_by_three_and_sum(logs_df, attribute1,attribute2, attribute3):\n",
    "    grouped_logs = logs_df.groupby([attribute1, attribute2]).agg(frequency_phase_sum=(attribute3, 'sum')).reset_index()\n",
    "    grouped_logs = grouped_logs.rename(columns={'frequency_phase_sum': 'count'})        \n",
    "    return grouped_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a flexible plot\n",
    "def plot_logs_data(logs_df, sortattribute, x_column, y_column=None, plot_type='bar', title=None, pivot=False, agg_func='sum'):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # If pivot is True, create a pivot table to handle grouped data with multiple y values\n",
    "    if pivot and y_column:\n",
    "        pivot_df = logs_df.pivot_table(index=x_column, columns=y_column, values=sortattribute, aggfunc=agg_func).fillna(0)\n",
    "        \n",
    "        if plot_type == 'line':\n",
    "            ax = pivot_df.plot(kind='line', marker='o', title=title if title else \"\")\n",
    "            plt.xlabel(x_column)\n",
    "            plt.ylabel(sortattribute)\n",
    "        elif plot_type == 'bar':\n",
    "            ax = pivot_df.plot(kind='bar', stacked=True, title=title if title else \"\")\n",
    "            plt.xlabel(x_column)\n",
    "            plt.ylabel(sortattribute)\n",
    "        else:\n",
    "            print(\"Pivoted data only supports 'line' and 'bar' plot types.\")\n",
    "    else:\n",
    "        # Standard plot handling\n",
    "        if plot_type == 'bar':\n",
    "            if y_column:\n",
    "                ax = logs_df.groupby(x_column)[y_column].agg(agg_func).plot(kind='bar', color='skyblue')\n",
    "            else:\n",
    "                ax = logs_df[x_column].value_counts().plot(kind='bar', color='skyblue')\n",
    "            plt.xlabel(x_column)\n",
    "            plt.ylabel(sortattribute if not y_column else y_column)\n",
    "        \n",
    "        elif plot_type == 'line' and y_column:\n",
    "            ax = logs_df.groupby(x_column)[y_column].agg(agg_func).plot(kind='line', marker='o')\n",
    "            plt.xlabel(x_column)\n",
    "            plt.ylabel(y_column)\n",
    "        \n",
    "        elif plot_type == 'scatter' and y_column:\n",
    "            plt.scatter(logs_df[x_column], logs_df[y_column], alpha=0.5, color='purple')\n",
    "            plt.xlabel(x_column)\n",
    "            plt.ylabel(y_column)\n",
    "            ax = plt.gca()\n",
    "        \n",
    "        elif plot_type == 'hist':\n",
    "            ax = logs_df[x_column].plot(kind='hist', bins=15, color='skyblue', edgecolor='black')\n",
    "            plt.xlabel(x_column)\n",
    "            plt.ylabel('Frequency')\n",
    "    \n",
    "    # Set the plot title\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    \n",
    "    # Add individual count labels on each bar\n",
    "    if plot_type == 'bar':\n",
    "        for container in ax.containers:\n",
    "            ax.bar_label(container, fmt='%d', label_type='center')\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data as a table using matplotlib, with an option to select specific columns\n",
    "def plot_table(df, columns=None, title=\"Table Plot\", col_widths=None, fontsize=10):\n",
    "    if columns:\n",
    "        df = df[columns]\n",
    "    \n",
    "    plt.figure(figsize=(len(df.columns) * 2, len(df) * 0.4))\n",
    "    ax = plt.gca()\n",
    "    ax.axis('off')\n",
    "    \n",
    "    table = plt.table(cellText=df.values,\n",
    "                      colLabels=df.columns,\n",
    "                      cellLoc='center',\n",
    "                      loc='center',\n",
    "                      colWidths=col_widths or [0.2] * len(df.columns))\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(fontsize)\n",
    "    table.scale(1, 1.5)\n",
    "    \n",
    "    plt.title(title, fontsize=fontsize + 2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Generated Log Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Count per Fault-Target Across Processing Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename count columns to represent each processing stage\n",
    "collected_logs_count = count_logs(collected_logs_df, 'fault-target')\n",
    "templated_logs_count = templated_logs_df.groupby('fault-target')['templated_message'].nunique().reset_index(name='count')\n",
    "difference_logs_count = difference_logs_df.groupby('fault-target')['templated_message'].nunique().reset_index(name='count')\n",
    "generalized_logs_count = count_logs(generalized_logs_df, 'fault-target')\n",
    "\n",
    "collected_logs_count = collected_logs_count.rename(columns={'count': 'Preparation'})\n",
    "templated_logs_count = templated_logs_count.rename(columns={'count': 'Generalization'})\n",
    "difference_logs_count = difference_logs_count.rename(columns={'count': 'Difference'})\n",
    "generalized_logs_count = generalized_logs_count.rename(columns={'count': 'Enrichment'})\n",
    "\n",
    "# Merge the data based on 'fault-target' attribute\n",
    "merged_counts = collected_logs_count.merge(templated_logs_count, on='fault-target', how='outer') \\\n",
    "                                    .merge(difference_logs_count, on='fault-target', how='outer') \\\n",
    "                                    .merge(generalized_logs_count, on='fault-target', how='outer')\n",
    "                                    \n",
    "# Fill NaN with 0 and sort by descending order\n",
    "merged_counts = merged_counts.fillna(0)\n",
    "merged_counts = merged_counts.sort_values(by='Enrichment', ascending=False)\n",
    "print(merged_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Count per Component Across Processing Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename count columns to represent each processing stage\n",
    "collected_logs_count_c = count_logs(collected_logs_df, 'component')\n",
    "templated_logs_count_c = templated_logs_df.groupby('component')['templated_message'].nunique().reset_index(name='count')\n",
    "difference_logs_count_c = difference_logs_df.groupby('component')['templated_message'].nunique().reset_index(name='count')\n",
    "generalized_logs_count_c = count_logs(generalized_logs_df, 'component')\n",
    "\n",
    "collected_logs_count_c = collected_logs_count_c.rename(columns={'count': 'Preparation'})\n",
    "templated_logs_count_c = templated_logs_count_c.rename(columns={'count': 'Generalization'})\n",
    "difference_logs_count_c = difference_logs_count_c.rename(columns={'count': 'Difference'})\n",
    "generalized_logs_count_c = generalized_logs_count_c.rename(columns={'count': 'Enrichment'})\n",
    "\n",
    "# Merge the data on 'component' attribute\n",
    "merged_counts = collected_logs_count_c.merge(templated_logs_count_c, on='component', how='outer') \\\n",
    "                                    .merge(difference_logs_count_c, on='component', how='outer') \\\n",
    "                                    .merge(generalized_logs_count_c, on='component', how='outer')\n",
    "                                    \n",
    "# Fill NaN with 0 and sort by descending order\n",
    "merged_counts = merged_counts.fillna(0)\n",
    "merged_counts = merged_counts.sort_values(by='Enrichment', ascending=False)\n",
    "print(merged_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Fault Injection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Count per Fault-Target per Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of faults using 'fault-target' and 'component' attributes\n",
    "generalized_logs_fault_count = group_by_and_count(generalized_logs_df, 'fault-target', 'component')\n",
    "generalized_logs_fault_count = generalized_logs_fault_count.rename(columns={'count': 'Log Count'})\n",
    "generalized_logs_fault_count = generalized_logs_fault_count.sort_values(by='Log Count', ascending=False)\n",
    "\n",
    "plot_logs_data(\n",
    "    logs_df=generalized_logs_fault_count,\n",
    "    sortattribute='Log Count',\n",
    "    x_column='fault-target',\n",
    "    y_column='component',\n",
    "    plot_type='bar',\n",
    "    title=None,\n",
    "    pivot=True,\n",
    "    agg_func='sum'\n",
    ")\n",
    "print(generalized_logs_fault_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Count per Component per Fault-Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of faults using 'component' and 'fault-target' attributes\n",
    "generalized_logs_component_count = group_by_and_count(generalized_logs_df,  'component', 'fault-target')\n",
    "generalized_logs_component_count = generalized_logs_component_count.rename(columns={'count': 'Log Count'})\n",
    "generalized_logs_component_count = generalized_logs_component_count.sort_values(by='Log Count', ascending=False)\n",
    "\n",
    "plot_logs_data(\n",
    "    logs_df=generalized_logs_component_count,\n",
    "    sortattribute='Log Count',\n",
    "    x_column='component',\n",
    "    y_column='fault-target',\n",
    "    plot_type='bar',\n",
    "    title=None,\n",
    "    pivot=True,\n",
    "    agg_func='sum'\n",
    ")\n",
    "print(generalized_logs_component_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriched Attribute Percentage per Fault-Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on attributes\n",
    "filtered_df = generalized_logs_df[\n",
    "    (generalized_logs_df['component-specific'] == True) |\n",
    "    (generalized_logs_df['fault-specific'] == True) |\n",
    "    (generalized_logs_df['rare'] == True) |\n",
    "    (generalized_logs_df['direct_cause'] == True) |\n",
    "    (generalized_logs_df['phase'] == 'crash')\n",
    "]\n",
    "\n",
    "enriched_attributes = ['component-specific', 'fault-specific', 'phase', 'rare', 'direct_cause']\n",
    "enriched_counts_t = pd.DataFrame()\n",
    "\n",
    "# Calculate the count of previously filtered rows and group them using the 'fault-target' attribute\n",
    "for attribute in enriched_attributes:\n",
    "    if attribute == 'phase':\n",
    "        attribute_count = filtered_df[filtered_df[attribute] == 'crash'].groupby('fault-target').size().reset_index(name= 'crash')\n",
    "    else:\n",
    "        attribute_count = filtered_df[filtered_df[attribute] == True].groupby('fault-target').size().reset_index(name=attribute)   \n",
    "    if enriched_counts_t.empty:\n",
    "        enriched_counts_t = attribute_count\n",
    "    else:\n",
    "        enriched_counts_t = enriched_counts_t.merge(attribute_count, on='fault-target', how='outer')\n",
    "\n",
    "# Replace NaN values with 0\n",
    "enriched_counts_t = enriched_counts_t.fillna(0)\n",
    "\n",
    "# Calculate the total count of each fault-target, merge it with the enriched counts and sort by descending order\n",
    "total_fault_target_count = generalized_logs_df.groupby('fault-target').size().reset_index(name='total_count')\n",
    "enriched_counts_t_df = total_fault_target_count.merge(enriched_counts_t, on='fault-target', how='outer')\n",
    "enriched_counts_t_df = enriched_counts_t_df.sort_values(by='total_count', ascending=False)\n",
    "\n",
    "enriched_percentages_t = enriched_counts_t_df.copy()\n",
    "\n",
    "# Calculate the percentage of each attribute\n",
    "attributes_to_convert = ['component-specific', 'fault-specific', 'crash', 'rare', 'direct_cause']\n",
    "for attribute in attributes_to_convert:\n",
    "    enriched_percentages_t[f'{attribute}'] = ((enriched_percentages_t[attribute] / enriched_percentages_t['total_count']) * 100).round(1)\n",
    "enriched_percentages_t = enriched_percentages_t[['fault-target', 'total_count'] + [f'{attr}' for attr in attributes_to_convert]]\n",
    "\n",
    "print(enriched_percentages_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriched Attribute Percentage per Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on attributes\n",
    "filtered_df = generalized_logs_df[\n",
    "    (generalized_logs_df['component-specific'] == True) |\n",
    "    (generalized_logs_df['fault-specific'] == True) |\n",
    "    (generalized_logs_df['rare'] == True) |\n",
    "    (generalized_logs_df['direct_cause'] == True) |\n",
    "    (generalized_logs_df['phase'] == 'crash')\n",
    "]\n",
    "\n",
    "enriched_attributes = ['component-specific', 'fault-specific', 'phase', 'rare', 'direct_cause']\n",
    "enriched_counts_r = pd.DataFrame()\n",
    "\n",
    "# Calculate the count of previously filtered rows and group them using the 'crash' attribute\n",
    "for attribute in enriched_attributes:\n",
    "    if attribute == 'phase':\n",
    "        attribute_count = filtered_df[filtered_df[attribute] == 'crash'].groupby('component').size().reset_index(name= 'crash')\n",
    "    else:\n",
    "        attribute_count = filtered_df[filtered_df[attribute] == True].groupby('component').size().reset_index(name=attribute)\n",
    "    if enriched_counts_r.empty:\n",
    "        enriched_counts_r = attribute_count\n",
    "    else:\n",
    "        enriched_counts_r = enriched_counts_r.merge(attribute_count, on='component', how='outer')\n",
    "\n",
    "# Replace NaN values with 0\n",
    "enriched_counts_r = enriched_counts_r.fillna(0)\n",
    "\n",
    "# Calculate the total count of each component, merge it with the enriched counts and sort by descending order\n",
    "total_fault_target_count = generalized_logs_df.groupby('component').size().reset_index(name='total_count')\n",
    "enriched_counts_r_df = total_fault_target_count.merge(enriched_counts_r, on='component', how='outer')\n",
    "enriched_counts_r_df = enriched_counts_r_df.sort_values(by='total_count', ascending=False)\n",
    "\n",
    "enriched_percentages_r = enriched_counts_r_df.copy()\n",
    "\n",
    "# Calculate the percentage of each attribute\n",
    "attributes_to_convert = ['component-specific', 'fault-specific', 'crash', 'rare', 'direct_cause']\n",
    "for attribute in attributes_to_convert:\n",
    "    enriched_percentages_r[f'{attribute}'] = ((enriched_percentages_r[attribute] / enriched_percentages_r['total_count']) * 100).round(1)\n",
    "enriched_percentages_r = enriched_percentages_r[['component', 'total_count'] + [f'{attr}' for attr in attributes_to_convert]]\n",
    "\n",
    "print(enriched_percentages_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Anomaly determination Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Count per Fault-Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows where 'determination' contains 'anomaly'\n",
    "anomalies_df_f = determination_logs_df[determination_logs_df['determination'].str.contains('anomaly', case=False, na=False)]\n",
    "\n",
    "# Group by 'fault-target' attribute and count the anomalies\n",
    "anomaly_count_f = anomalies_df_f.groupby('fault-target').size().reset_index(name='anomaly_count')\n",
    "anomaly_count_f = anomaly_count_f.sort_values(by='anomaly_count', ascending=False)\n",
    "\n",
    "print(anomaly_count_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Count per Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows where 'determination' contains 'anomaly'\n",
    "anomalies_df_c = determination_logs_df[determination_logs_df['determination'].str.contains('anomaly', case=False, na=False)]\n",
    "\n",
    "# Group by 'fault-target' attribute and count the anomalies\n",
    "anomaly_count_c = anomalies_df_c.groupby('component').size().reset_index(name='anomaly_count')\n",
    "anomaly_count_c = anomaly_count_c.sort_values(by='anomaly_count', ascending=False)\n",
    "\n",
    "print(anomaly_count_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'determination' contains 'anomaly' in the determination_logs_df\n",
    "anomalies_df = determination_logs_df[determination_logs_df['determination'].str.contains('anomaly', case=False, na=False)]\n",
    "\n",
    "# Group by 'component' attribute and count the anomalies\n",
    "anomaly_count_per_component = anomalies_df.groupby('component').size().reset_index(name='anomaly_count')\n",
    "\n",
    "print(anomaly_count_per_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Count per fault-target per component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of faults using 'component' and 'fault-target' attributes\n",
    "\n",
    "anomalies_df = determination_logs_df[determination_logs_df['determination'].str.contains('anomaly', case=False, na=False)]\n",
    "anomaly_distribution = anomalies_df.groupby(['fault-target', 'component']).size().reset_index(name='anomaly_count')\n",
    "anomaly_distribution = anomaly_distribution.sort_values(by='anomaly_count', ascending=False)\n",
    "\n",
    "plot_logs_data(\n",
    "    logs_df=anomaly_distribution,      \n",
    "    x_column='fault-target',           \n",
    "    y_column='component',              \n",
    "    plot_type='bar',                   \n",
    "    title=None,\n",
    "    pivot=True,                        \n",
    "    agg_func='sum',                     \n",
    "    sortattribute='anomaly_count'\n",
    ")\n",
    "\n",
    "print(anomaly_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Determination Counts and Attributes Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on attributes\n",
    "filtered_df = determination_logs_df[\n",
    "    (determination_logs_df['component-specific'] == True) |\n",
    "    (determination_logs_df['fault-specific'] == True) |\n",
    "    (determination_logs_df['phase'] == 'crash') |\n",
    "    (determination_logs_df['rare'] == True) |\n",
    "    (determination_logs_df['direct_cause'] == True)\n",
    "]\n",
    "\n",
    "enriched_attributes = ['component-specific', 'fault-specific', 'phase', 'rare', 'direct_cause']\n",
    "enriched_counts_r = pd.DataFrame()\n",
    "\n",
    "# Calculate the count of previously filtered rows and group them using the 'crash' attribute\n",
    "for attribute in enriched_attributes:\n",
    "    if attribute == 'phase':\n",
    "        attribute_count = filtered_df[filtered_df[attribute] == 'crash'].groupby('determination').size().reset_index(name= 'crash')\n",
    "    else:\n",
    "        attribute_count = filtered_df[filtered_df[attribute] == True].groupby('determination').size().reset_index(name=attribute)\n",
    "    if enriched_counts_r.empty:\n",
    "        enriched_counts_r = attribute_count\n",
    "    else:\n",
    "        enriched_counts_r = enriched_counts_r.merge(attribute_count, on='determination', how='outer')\n",
    "\n",
    "# Replace NaN values with 0\n",
    "enriched_counts_r = enriched_counts_r.fillna(0)\n",
    "\n",
    "# Calculate the total count of each component, merge it with the enriched counts and sort by descending order\n",
    "total_fault_target_count = determination_logs_df.groupby('determination').size().reset_index(name='total_count')\n",
    "enriched_counts_r_df = total_fault_target_count.merge(enriched_counts_r, on='determination', how='outer')\n",
    "enriched_counts_r_df = enriched_counts_r_df.sort_values(by='total_count', ascending=False)\n",
    "\n",
    "enriched_percentages_r = enriched_counts_r_df.copy()\n",
    "\n",
    "# Calculate the percentage for each enriched attribute by dividing by total_count and multiplying by 100\n",
    "attributes_to_convert = ['component-specific', 'fault-specific', 'crash', 'rare', 'direct_cause']\n",
    "for attribute in attributes_to_convert:\n",
    "    enriched_percentages_r[f'{attribute}'] = ((enriched_percentages_r[attribute] / enriched_percentages_r['total_count']) * 100).round(1)\n",
    "enriched_percentages_r = enriched_percentages_r[['determination', 'total_count'] + [f'{attr}' for attr in attributes_to_convert]]\n",
    "\n",
    "\n",
    "print(enriched_percentages_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Determination Counts per Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # Perform aggregation grouped on columns: 'determination', 'component'\n",
    "    df = df[df['determination'].str.contains(\"anomaly\", regex=False, na=False, case=False)]\n",
    "    df = df.groupby(['determination', 'component']).agg(determination_count=('determination', 'count')).reset_index()\n",
    "    return df\n",
    "\n",
    "df_clean = clean_data(determination_logs_df.copy())\n",
    "df_clean = df_clean.sort_values(by='determination_count', ascending=False)\n",
    "\n",
    "plot_logs_data(\n",
    "    logs_df=df_clean,      \n",
    "    x_column='determination',          \n",
    "    y_column='component',             \n",
    "    plot_type='bar',                  \n",
    "    pivot=True,                       \n",
    "    agg_func='sum',                   \n",
    "    sortattribute='determination_count'\n",
    ")\n",
    "\n",
    "df_clean\n",
    "print(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Determination Counts per Fault-Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # Perform aggregation grouped on columns: 'determination', 'fault-target'\n",
    "    df = df[df['determination'].str.contains(\"anomaly\", regex=False, na=False, case=False)]\n",
    "    df = df.groupby(['determination', 'fault-target']).agg(determination_count=('determination', 'count')).reset_index()\n",
    "    return df\n",
    "\n",
    "df_clean = clean_data(determination_logs_df.copy())\n",
    "df_clean = df_clean.sort_values(by='determination_count', ascending=False)\n",
    "\n",
    "plot_logs_data(\n",
    "    logs_df=df_clean, \n",
    "    x_column='determination', \n",
    "    y_column='fault-target',  \n",
    "    plot_type='bar',          \n",
    "    pivot=True,               \n",
    "    agg_func='sum',           \n",
    "    sortattribute='determination_count'\n",
    ")\n",
    "\n",
    "print(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Cause Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # Filter rows based on columns: 'determination', 'direct_cause'\n",
    "    df = df[(df['determination'].str.contains(\"anomaly\", regex=False, na=False, case=False)) & (df['direct_cause'] == True)]\n",
    "    return df\n",
    "\n",
    "df_clean = clean_data(determination_logs_df.copy())\n",
    "print(df_clean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
